Attack of the giant ANTS - Advanced Namespace ToolS

The Advanced Namespace ToolS are a collection of plan 9 software designed to serve many purposes. To enumerate:

1. Reliability, Resiliency, Recoverability - make grids of Plan 9 machines tolerant to hardware and software failures.
2. Flexibility and Configurability - make it easy to change what role a system is playing in a grid, to add in new systems, and retire old ones, without disrupting the user environment.
3. Multiplexing namespaces on one machine - the standard Plan 9 installer sets up a Plan 9 environment with a namespace organized to appear similar to a traditional unix environment, and all software on the machine operates within a similar namespace. This is purely conventional, and the Plan 9 kernel is happy to run multiple user environments with radically different namespaces on the same machine. The most striking application of this is running both 9front userland and standard Bell Labs userland on the same box at the same time, one available via CPU(1) and one via the terminal's physical gui and hardware i/o devices.
4. Ad-hoc namespaces built from multiple machines - The converse of the previous goal is the ability to easily create an envrionment built from diverse and distinct services on different machines. This is part of "standard" plan 9 but the existing tools and scripts don't provide much in the way of high-level semantics for en masse namespace operations. 
5. Data sharing and persistence - Multiple machines should be able to work easily with the same data, even dynamic, flowing data, and being able to study and replicate past data flows is useful. 
6. Adminstration - It should be easy for the user to administer all the machines used to create the workspace. Administration should be contained away from the main user environment, but constantly and easily accessible.
7. Automatic backup/replication and recovery. Redundant usable copies of data should be made automatically, and reverting to an earlier environment should be painless and fast.

The goal is to present to the user a computing and data environment abstracted as far away from the underlying hardware as possible, by leveraging the power of namespaces and network transparency, and make that computing and data environment as "indestructible" as possible. No matter what happens to the hardware of an individual machine or machines, the user can continue working with as little hassle as possible. Certainly, there are limits. A running application on a machine that dies will probably result in the loss of some state and perhaps some unsaved data, but this need not be more than a minor annoyance in the overall computing environment.

Background: I've been working with multiple-machine plan 9 systems for about 5 years, trying to experience the "original design" of separate services acquired from the network via a terminal. I have found this to be an environment with desirable properties, many of them as described in the original papers. I also encountered some obstacles as a home user trying to deploy a true distributed plan 9 environment. In the original plan 9 design, the "infrastructure" of file and cpu servers was intended to be installed in a professional managed datacenter. The default assumptions, though somewhat adjusted over the years, remain best suited for a world where a given hardware and network configuration has a lifespan measured in years. In a home network, things may often change on a timescale of weeks, days, or even hours. The user is likely to turn off and turn on machines more often, shift both public and private nat ips, and in general operate the machines in a much less predictable way. Also, the hardware a home user is likely to be using for plan 9 is a mixture of older machines, virtual machines, and desktops hosting related software like plan9port. This is a very different reliability profile than professional datacenter hardware.

My experience as a home user building on top of older hardware mixed with virtual machines and making frequent changes to my network arrangement was that the user environment I had in plan 9 was amazing, but somewhat fragile. The grid architecture created dependencies between the different machines. If you have a backing store machine (venti) supporting a file server supporting a cpu server, the user environment breaks if there is any disruption of the machines and their network connections. At the time four years ago, Qemu virtualization also seemed less robust than now, and my VMs were prone to crashing if placed under significant load. Plan 9 was giving me a user environment that I loved, but I was also struggling with reliability issues - qemu VMs running fossil often corrupted their filesystem when crashing badly. 

It seemed to me that a grid of multiple machines should create the property of being more reliable and resilient than any of the components, and I was experiencing more of a "one fall, they all fall" experience. The more tightly I tried to bind my machines together by importing services from each other, the more fragile everything became. I wanted to cpurc on the machines to acquire services from the other machines on the grid, to put those "underneath" the user namespace so that when I cpu'd in, a multiple machine namespace would be built and waiting. Doing this kind of service acquisition from the main flow of control in cpurc though created system wide dependencies on those resources, and my user environment would freeze up if a read from a network mount didn't return. I tried using aan and recover, but those tools are for dealing with network-level disruptions, not a machine that dies and has to reboot.

Another issue I experienced working with my grid was the lack of higher-level tools for dealing with namespaces, and a general focus on creating a namespace which roughly mirrored conventional unix. It felt to me that the mechanism of namespaces was so brilliant and so flexible and open-ended that there was much more that could be done with manipulating namespaces and network transparency to build interesting environments. What I wanted was a way to "weave" a complicated namespace that unified different machines, but was also resilient to their failure and would replicate and secure my data without extra work. 

As I experimented with different partial solutions (just running 3 copies of everything, which was definitely part of the way forward) it became clear to me that there was a fundamental, and unnecessary, dependency that was at the root - so to speak - of my troubles. This was the dependency of the plan 9 kernel on a given root filesystem chosen at boot-time. When a running cpu server loses its root fileserver, it becomes "dead" - even though it experienced no failure or data corruption or any disruption in its normal function, it just lost a file descriptor. 

If the user at their terminal is using that cpu server, and also importing their $home from a different fileserver, the user's connection to their $home and working data set is disrupted, even though the $home fileserver is still fine - it is just the boot fileserver for the cpu server with the binaries that had a disruption. The failure of one system (the cpu root fileserver with the binaries) has caused the "failure" of 3 other working systems and ended the workflow of the user until the cpu is rebooted. The cpu kernel is still running fine - why does it need to reboot because a user-space program lost a file descriptor?

The solution to all these challenges seemed clear - remove the dependency of the kernel on the root filesystem. Once the kernel is loaded into memory, it can "stand back" from userland, and instruct userspace programs to dial each other and create a namespace for the user, but the main flow of control of the master process on the system should not depend on any filedescriptor acquired from an outside system. That way, anything can die and the master flow of control never freezes trying to read from a dead fd. Here is the namespace of the "master process" on a rootless box:

bind #c /dev
bind #ec /dev
bind -bc #e /env
bind -c #s /srv
bind -a #u /dev
bind / /
bind #d /fd
bind #p /proc
bind -a capdevice /dev
bind -a #S /dev
bind /net /net
bind -a #l /net
bind -a #I /net
bind /bin /bin
bind -a /boot /bin
cd /

Nothing can kill this shell except the failure of the kernel itself. The tools compiled into /boot allow it to fork off new environments rooted to disk or network resources without connecting to it themself. This master process can be thought of as a "namespace hypervisor" which stays behind the scenes creating different namespace environments. We arrange for this master process to exist by rewriting boot and init into a single program which sets up the kernel-only environment. Once this is done, the script "plan9rc" is forked off to take control of setting up a user environment of services on top of the kernel, but leaving the main flow of control unattached to those resources.

plan9rc is a script which acts as a general purpose "environment launcher" for plan 9 environments. It takes over the work usually done by the kernel to attach a root fileserver and launch a cpurc or termrc. It also provides an optional "interactive" mode which has offers complete user control of all bootup parameters. Once an environment has been "launched" it saves the parameters used to its local ramdisk, allowing the same environment to be spawned again non-interactively. 

plan9rc is written to be compatible with existing installations and plan9.ini values to a great extent. It is focused on pc bootup and handles tcp booting, fossil, venti, and cfs in the same manner as the conventional kernel. USB boot is currently untested but I will be working on making sure it is supported as well. plan9rc creates the foundation environment to allow termrc or cpurc to be run as usual, and an existing standard plan 9 install should perform as usual when launched in this fashion. In this way the rootless kernel acts like a foundation lift for a building that lifts the entire structure and installs a new floor at ground level.

How do we access and use this new foundation level to give us control over the system? This is done by a small cpurc-equivalent script named "initskel" that may be run by plan9rc. The initskel creates a miniature user environment rooted on a small ramdisk, so it is also independent of any non-kernel resources. This environment has a much richer namespace than the core kernel control process but it is still independent of any external resources. Compiled into /boot are enough tools to allow this namespace to run a cpu listener (on a non-standard port). Here is what the ns looks like when we cpu in on port 17020:

bind  /root /root 
mount -ac '#s/ramboot' /root 
bind  / / 
bind -a /root / 
mount -a '#s/ramboot' / 
bind -c /root/mnt /mnt 
bind  '#c' /dev 
bind  '#d' /fd 
bind -c '#e' /env 
bind  '#p' /proc 
bind -c '#s' /srv 
bind -a '#¤' /dev 
bind -a '#S' /dev 
bind  /n /n 
mount -a '#s/slashn' /n 
mount -a '#s/factotum' /mnt 
bind  /bin /bin 
bind -b /boot /bin 
bind -a /root/bin /bin 
bind  /boot /boot 
bind -a /root/bin /boot 
bind  /net /net 
bind -a '#l' /net 
bind -a '#I' /net 
mount -a '#s/cs' /net 
mount -a '#s/dns' /net 
mount -c '#s/hubfs' /n/hubfs 
mount -c '#D/ssl/0/data' /mnt/term 
bind -a /usr/bootes/bin/rc /bin 
bind -a /usr/bootes/bin/386 /bin 
bind -c /usr/bootes/tmp /tmp 
bind -a /mnt/term/mnt/wsys /dev 
bind  /mnt/term/dev/cons /dev/cons 
bind  /mnt/term/dev/consctl /dev/consctl 
bind -a /mnt/term/dev /dev 
cd /usr/bootes

This namespace is a very important namespace in the structure of the grid. It exists on every single machine, created under whatever kind of cpurc or termrc they run. This environment is a perfectly user-friendly namespace, unlike the pure kernel namespace with no ramdisk attached. In fact, depending on what is compiled into boot and the optional tools.tgz contained in 9fat which may also be added to the ramdisk, this environment, while slightly spartan (no manpages, only 1 or 2 fonts 9 in /lib, etc) is in fact sufficient for many tasks. Furthermore, since you are cpu-ing in as usual to a new flow of control, you can freely acquire new resources from here without fear. If your cpu-d in environment breaks, it hasnt harmed the flow of control it spawned from, the service and utility namespace will be the same on next cpu in.

To aid in working using the service namespace as a base, scripts are provided to provide forms of re-rooting. Some of the simplest are "addwrroot" and "importwrroot" which target external file or cpu servers and acquire their resources and bind them in locally while still keeping the ramboot root. The binds are to acquire the binaries, lib and sys, and usr directories from the remote system. If you wish to "fully" attach to a new root while mainting your drawterm and cpu connection, the script "rerootwin" provides this functionality. This is one of the most important tools for fast transformation of a user sub-environment. rerootwin works by "saving" the active devices with srvfs of /mnt/term and /mnt/wsys, then it uses a custom namespace file to root to a named /srv or network machine, and then re-acquire the original devices from the srvfs to allow the user to remain in full control and continue to run graphical applications in that window. Here is what the namespace looks like after cpu into a "service namespace", start rio, then open a window and run "rerootwin" targeting a different machine on the network:

bind  '#c' /dev 
bind  '#d' /fd 
bind -c '#e' /env 
bind  '#p' /proc 
bind -c '#s' /srv 
bind -a '#¤' /dev 
bind -a '#S' /dev 
bind  /net /net 
bind -a '#l' /net 
bind -a '#I' /net 
bind  /net.alt /net.alt 
mount -a '#s/slashn' /net.alt 
mount -c '#s/oldterm.967' /net.alt/oldterm.967 
mount -c '#s/oldwsys.967' /net.alt/oldwsys.967 
bind  /net.alt/oldterm.967/dev/cons /dev/cons 
bind  /net.alt/oldterm.967/dev/consctl /dev/consctl 
bind -a /net.alt/oldterm.967/dev /dev 
mount -b '#s/oldwsys.967' /dev 
bind  /mnt /mnt 
mount -a '#s/factotum' /mnt 
bind  /root /root 
mount -ac '#s/gridfront' /root 
bind  / / 
bind -a /root / 
mount -a '#s/gridfront' / 
bind -b /root/mnt /mnt 
bind  /bin /bin 
bind -b /boot /bin 
bind -a /386/bin /bin 
bind -a /rc/bin /bin 
bind  /n /n 
mount -a '#s/slashn' /n 
mount -a '#s/cs' /net 
mount -a '#s/dns' /net 
mount -c '#s/hubfs' /n/hubfs 
bind -c /usr/bootes/tmp /tmp 
cd /usr/bootes

Using the rerootwin script in combination with the service namespace makes the cpu server a true cpu server, because you are no longer using the cpu's root at all. It is truly just providing execution resources at the junction of two totally independent systems. By cpu into the service namespace and then rerootwin to different file servers, the user environment is equivalent to one rooted conventionally to that environment, but without the depency. If the re-rooted environment breaks, the user's active workspace on the cpu outside the re-rooted window is unharmed. 